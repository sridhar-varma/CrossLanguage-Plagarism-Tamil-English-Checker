{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjfzxXZLHed_",
        "outputId": "40e7e687-35d1-4640-f946-bc53763bf73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 700 (delta 280), reused 341 (delta 238), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (700/700), 2.64 MiB | 6.18 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1399, done.\u001b[K\n",
            "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 1399 (delta 135), reused 147 (delta 120), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1399/1399), 9.57 MiB | 4.58 MiB/s, done.\n",
            "Resolving deltas: 100% (745/745), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 16.79 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 597, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 597 (delta 8), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (597/597), 252.23 KiB | 5.87 MiB/s, done.\n",
            "Resolving deltas: 100% (357/357), done.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeYW2BJhlJvx",
        "outputId": "2f8011e1-e8be-4430-c9ab-d062df74c9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (10.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0 sacremoses-0.1.1 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.66.2)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49170 sha256=055007aae3c58e0788795a39ef08571e2983328857bf916f96e819f5800b06f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6a427871af39307882d55dfaaabcc2bff48ac653aaf84bf0e63aaa1c2c2308df\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3338 sha256=6267b1f1c8041de64447ba75080e5028f52b6c1b177fbf0f4adfdaa128aa41db\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6147 sha256=aa06ed1fc60ecba1e23bce5eb510b18e894da745c879118409a7a7c2fbf3254a\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35143, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 35143 (delta 52), reused 67 (delta 38), pack-reused 35057\u001b[K\n",
            "Receiving objects: 100% (35143/35143), 25.18 MiB | 18.92 MiB/s, done.\n",
            "Resolving deltas: 100% (25529/25529), done.\n",
            "/content/fairseq\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.8)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.25.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.12.25)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.2)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (23.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.9.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19832220 sha256=365bcae2f44e5b3335958f09c58dcd1e1e349180c8a8deb23ede61f1e2b6c655\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h_1b_dql/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=501d73add0a14afb97b6b175dcb7bca0e98b3788e3d564277c4f4b21bd27f789\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Collecting torch==2.2.0 (from xformers)\n",
            "  Using cached torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0->xformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->xformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->xformers)\n",
            "  Using cached triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->xformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0->xformers) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 triton-2.2.0 xformers-0.0.24\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TktUu9NW_PLq",
        "outputId": "576e4502-fa41-44ed-9a9e-b7b0ee3a9572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "WARNING:xformers:Triton is not available, some optimizations will not be enabled.\n",
            "This is just a warning: triton is not available\n"
          ]
        }
      ],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_4JxNdRlPQB",
        "outputId": "c39a6de3-2324-4837-e5e4-78399218961a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-16 07:57:52--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 101.53.152.30, 101.53.152.33, 164.52.210.97, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|101.53.152.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4759117228 (4.4G) [application/zip]\n",
            "Saving to: ‘indic2en.zip’\n",
            "\n",
            "indic2en.zip        100%[===================>]   4.43G  11.8MB/s    in 7m 15s  \n",
            "\n",
            "2024-02-16 08:05:09 (10.4 MB/s) - ‘indic2en.zip’ saved [4759117228/4759117228]\n",
            "\n",
            "Archive:  indic2en.zip\n",
            "   creating: indic-en/\n",
            "   creating: indic-en/vocab/\n",
            "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: indic-en/vocab/vocab.SRC  \n",
            "  inflating: indic-en/vocab/vocab.TGT  \n",
            "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: indic-en/final_bin/\n",
            "  inflating: indic-en/final_bin/preprocess.log  \n",
            "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: indic-en/model/\n",
            "  inflating: indic-en/model/checkpoint_best.pt  \n",
            "/content/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# download the indictrans model\n",
        "\n",
        "\n",
        "# downloading the indic-en model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "!unzip indic2en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "# !unzip en2indic.zip\n",
        "\n",
        "# # downloading the indic-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# !unzip m2m.zip\n",
        "\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTnWbHqY01-B",
        "outputId": "3d159723-a342-459e-d7d0-766b89bd2685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ],
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "indic2en_model = Model(expdir='../indic-en')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlKsYGDTUzSH",
        "outputId": "38317918-c94d-46df-b761-5a91f66f6ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTp2NOgQ__sB",
        "outputId": "70a4a555-cdcc-484f-a289-57efb1f5f586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 2448.99it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['He seems to know us.',\n",
              " 'We will follow you or continue to follow you',\n",
              " 'If you develop these symptoms in someone close to you, staying at home can help prevent the spread of Corona virus infection.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "ta_sents = ['அவனுக்கு நம்மைப் தெரியும் என்று தோன்றுகிறது',\n",
        "            \"நாங்கள் உன்னைத் பின்பற்றுவோம் (அ) தொடர்வோம். \",\n",
        "            'உங்களுக்கு உங்கள் அருகில் இருக்கும் ஒருவருக்கோ இத்தகைய அறிகுறிகள் தென்பட்டால், வீட்டிலேயே இருப்பது, கொரோனா வைரஸ் தொற்று பிறருக்கு வராமல் தடுக்க உதவும்.']\n",
        "\n",
        "\n",
        "indic2en_model.batch_translate(ta_sents, 'ta', 'en')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFXrCNZGEN7Z",
        "outputId": "aea8012b-ec77-4973-b50e-14115e531b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 3558.26it/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ta_paragraph = \"\"\"இத்தொற்றுநோய் உலகளாவிய சமூக மற்றும் பொருளாதார சீர்குலைவை ஏற்படுத்தியுள்ளது.இதனால் பெரும் பொருளாதார மந்தநிலைக்குப் பின்னர் உலகளவில் மிகப்பெரிய மந்தநிலை ஏற்பட்டுள்ளது. இது விளையாட்டு,மத, அரசியல் மற்றும் கலாச்சார நிகழ்வுகளை ஒத்திவைக்க அல்லது ரத்து செய்ய வழிவகுத்தது.\n",
        "அச்சம் காரணமாக முகக்கவசம், கிருமிநாசினி உள்ளிட்ட பொருட்களை அதிக நபர்கள் வாங்கியதால் விநியோகப் பற்றாக்குறை ஏற்பட்டது.\"\"\"\n",
        "\n",
        "file1data=indic2en_model.translate_paragraph(ta_paragraph, 'ta', 'en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyUjJaTQeqyK"
      },
      "outputs": [],
      "source": [
        "file2data=\"Early in the pandemic, Australia and New Zealand were widely praised for their response to Covid but both saw an increase in cases at the start of the year.So far, Australia has seen more than 8 million confirmed cases, with spikes in January and April. It has also recorded more than 10,000 confirmed deaths due to coronavirus.Daily cases are rising in New Zealand following a fall. So far it recorded 1.3 million cases, but only 1,535 deaths.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPu3uw0tL5ql"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCgZjiShL5uC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "def findFileSimilarity(inputQuery, database):\n",
        "\n",
        "\tuniversalSetOfUniqueWords = []\n",
        "\tmatchPercentage = 0\n",
        "\n",
        "\tlowercaseQuery = inputQuery.lower()\n",
        "\ten_stops = set(stopwords.words('english'))\n",
        "\n",
        "\t# Replace punctuation by space and split\n",
        "\tqueryWordList = re.sub(\"[^\\w]\", \" \", lowercaseQuery).split()\n",
        "\t# queryWordList = map(str, queryWordList)\t\t\t\t\t#This was causing divide by zero error\n",
        "\n",
        "\tfor word in queryWordList:\n",
        "\t\tif word not in universalSetOfUniqueWords:\n",
        "\t\t\tuniversalSetOfUniqueWords.append(word)\n",
        "\n",
        "\tdatabase1 = database.lower()\n",
        "\n",
        "\t# Replace punctuation by space and split\n",
        "\tdatabaseWordList = re.sub(\"[^\\w]\", \" \", database1).split()\n",
        "\t# databaseWordList = map(str, databaseWordList)\t\t\t#And this also leads to divide by zero error\n",
        "\n",
        "\tfor word in databaseWordList:\n",
        "\t\tif word not in universalSetOfUniqueWords:\n",
        "\t\t\tuniversalSetOfUniqueWords.append(word)\n",
        "\n",
        "\tfor word in universalSetOfUniqueWords:\n",
        "\t\tif word in en_stops:\n",
        "\t\t\tuniversalSetOfUniqueWords.remove(word)\n",
        "\n",
        "\tqueryTF = []\n",
        "\tdatabaseTF = []\n",
        "\n",
        "\tfor word in universalSetOfUniqueWords:\n",
        "\t\tqueryTfCounter = 0\n",
        "\t\tdatabaseTfCounter = 0\n",
        "\n",
        "\t\tfor word2 in queryWordList:\n",
        "\t\t\tif word == word2:\n",
        "\t\t\t\tqueryTfCounter += 1\n",
        "\t\tqueryTF.append(queryTfCounter)\n",
        "\n",
        "\t\tfor word2 in databaseWordList:\n",
        "\t\t\tif word == word2:\n",
        "\t\t\t\tdatabaseTfCounter += 1\n",
        "\t\tdatabaseTF.append(databaseTfCounter)\n",
        "\n",
        "\tdotProduct = 0\n",
        "\tfor i in range(len(queryTF)):\n",
        "\t\tdotProduct += queryTF[i]*databaseTF[i]\n",
        "\n",
        "\tqueryVectorMagnitude = 0\n",
        "\tfor i in range(len(queryTF)):\n",
        "\t\tqueryVectorMagnitude += queryTF[i]**2\n",
        "\tqueryVectorMagnitude = math.sqrt(queryVectorMagnitude)\n",
        "\n",
        "\tdatabaseVectorMagnitude = 0\n",
        "\tfor i in range(len(databaseTF)):\n",
        "\t\tdatabaseVectorMagnitude += databaseTF[i]**2\n",
        "\tdatabaseVectorMagnitude = math.sqrt(databaseVectorMagnitude)\n",
        "\n",
        "\tmatchPercentage = (float)(\n",
        "\t\tdotProduct / (queryVectorMagnitude * databaseVectorMagnitude))*100\n",
        "\n",
        "# \tprint (universalSetOfUniqueWords)\n",
        "# \tprint()\n",
        "# \tprint (databaseWordList)\n",
        "\n",
        "\n",
        "# \tprint (queryTF)\n",
        "# \tprint (databaseTF)\n",
        "\n",
        "\treturn matchPercentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkMhsXNxLUuW",
        "outputId": "577b7acc-a832-480b-a38b-6ed70b3b7790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "simi = findFileSimilarity(file1data,file2data )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGV3eJYtLUxy",
        "outputId": "756e5947-0772-4301-cc34-b089172c5f40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not potential plagarized\n"
          ]
        }
      ],
      "source": [
        "if simi>50:\n",
        "  print(\"potential plagarized\")\n",
        "else:\n",
        "  print(\"not potential plagarized\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZEZOJdTMAvD",
        "outputId": "355717b9-cf8b-4a79-82ef-d7b1f6a46c05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6369648372665395"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "simi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYuYBXsCVnEF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"\"\"திஸ் இஸ் அன் ஆர்டிக்ல் அபௌட் ஏபிஜே அப்துல் கலாம்,ஹூ இஸ் ஒன்ஸ் தி பிரசிடெண்ட் ஆஃப் இந்தியா.\"\"\"\n",
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')\n",
        "file2data=\"This is an article about APJ Abdul Kalam, Who is once the President of India.\"\n",
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkiuXVTI8RnK",
        "outputId": "fca0cad6-c812-4a94-c025-d90a29716a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 2242.94it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"இந்தியாவின் பிரதமரான மோடியைப் பற்றிய கட்டுரை இது.\"\n",
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')\n",
        "file2data=\"This is an article about APJ Abdul Kalam, Who is once the President of India.\"\n",
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcliVLk08Rpm",
        "outputId": "10e7e8a8-4770-47ae-8707-78cf75b90bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 1004.62it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31.622776601683793"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"நீங்கள் 4 + 3 ஐக் கூட்டினால் அது உங்களுக்கு 7 பதிலைத் தருகிறது, 8 அல்ல !!\"\n",
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')\n",
        "file2data=\"if you add 4+3 it gives you the answer 7,not 8 !!!\"\n",
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3L3oHy_8Rr1",
        "outputId": "c5828617-9e3d-4c1e-a0ef-cad31e0ff418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 4286.46it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87.03882797784892"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"நீங்கள் 5 + 8 ஐக் கூட்டினால் அது உங்களுக்கு 13, 8 அல்ல என்ற பதிலைத் தருகிறது!!\"\n",
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')\n",
        "file2data=\"if you add 4+3 it gives you the answer 7,not 8 !!!\"\n",
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFEWUPXq8RuM",
        "outputId": "6a21c6dd-30e8-4134-9a2e-98dbd59d6032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 4431.38it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72.16878364870323"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"2 சூன் 2014 நாளன்று ஆந்திரப் பிரதேசத்தின் வடகிழக்குப் பகுதி பிரிக்கப்பட்டு தெலுங்கானா என்ற புதிய மாநிலம் உருவாக்கப்பட்டது. இதனால் நீண்டகாலமாக ஆந்திராவின் தலைநகரமாக இருந்து வந்த ஐதராபாத் நகரம், தெலங்கானாவின் தலைநகரமாக மாறியது. எனினும் ஆந்திர பிரதேச மறுசீரமைப்பு சட்டம் 2014-இன்படி ஐதராபாத் ஆந்திரப் பிரதேசத்தின் சட்டப்பூர்வ தலைநகராக அதிகபட்சம் பத்தாண்டுகள்வரை நீடிக்கும். அதற்குள் புதிய தலைநகரமாக அமராவதி என்ற நகரம் உருவாக்கப்பட்டபிறகு அது சட்டப்படி ஆந்திரப் பிரதேசத்தின் தலைநகரமாக மாறும்.குசராத்தை அடுத்து ஆந்திரப் பிரதேசம், இந்தியாவின் 2-ஆவது மிக நீளமான கடற்கரை எல்லையைக் கொண்டுள்ளது. இது வடமேற்கில் தெலுங்கானா, வடகிழக்கில் சத்தீசுகர் மற்றும் ஒடிசா, மேற்கில் கருநாடகா மற்றும் தெற்கில் தமிழ்நாடு ஆகிய மாநிலங்களுடன் எல்லைகளைக் கொண்டுள்ளது. இதன் கிழக்கில் வங்காள விரிகுடா அமைந்துள்ளது. மேலும் புதுச்சேரி ஒன்றியப் பகுதியின் மாவட்டமான யானம் என்ற சிறிய பகுதி, ஆந்திர மாநிலத்தில் உள்ள கோதாவரி டெல்டாவில் காக்கிநாடா நகரத்தின் தெற்குப் பகுதியில் அமைந்துள்ளது.விசயவாடா, திருப்பதி, குண்டூர், காக்கிநாடா, நெல்லூர் மற்றும் கர்நூல் ஆகியன இம்மாநிலத்திலுள்ள ஏனைய பெரிய நகரங்களாகும்.\""
      ],
      "metadata": {
        "id": "Lwga0FHO8Rwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VltYT9-48Ryn",
        "outputId": "ded04b2f-9206-4733-f8bf-1272736aa8e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:00<00:00, 2682.54it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file2data=\"As per the 8th century BCE Rigvedic text Aitareya Brahmana, the Andhras left North India off the banks of the Yamuna river and migrated to South India. In the third century BCE, Andhra was a vassal kingdom of Ashoka of the Mauryan Empire. After his death, it became powerful and extended its empire to the whole Maratha country and beyond under the rule of the Satavahana dynasty. After that, the major rulers included the Pallavas, Eastern Chalukyas, Kakatiyas, Vijayanagara Empire, Qutb Shahi dynasty, Nizam dynasty, East India Company, and British Raj.The Eastern Ghats are a major dividing line separating coastal plains and peneplains. The coastal plains are part of Coastal Andhra. These are mostly delta regions formed by the Krishna, Godavari, and Penna rivers. Peneplains are part of Rayalaseema. 60% of the population is engaged in agriculture and related activities. Rice is the state's major food crop and staple food. The state contributes 10% of total fish production and over 70% of shrimp production in India. Industry sectors such as food products, non-metallic minerals, textiles, and pharmaceuticals are the top employment providers. The automotive sector accounts for 10% of India's auto exports. The state has about one-third of India's limestone reserves, large deposits of baryte and galaxy granite, and reserves of oil and natural gas.[11] Satish Dhawan Space Centre (SDSC), known as Sriharikota Range (SHAR), at the barrier island of Sriharikota in Tirupati district, is the satellite launching station of India.Some of the unique products from the state are Banganapalle mangoes, Bandar laddu, Kondaplli toys, Tirupati laddu, and saris made in Dharmavaram and Machilipatnam. Kuchipudi is the official dance form. Many composers of Carnatic music, like Annamacharya, Kshetrayya, and Tyagaraja, were from this region. The Tirumala Venkateswara temple near Tirupati is the most visited Hindu religious place in the world. The state is home to a variety of other pilgrimage centres and natural attractions.\""
      ],
      "metadata": {
        "id": "MTJcICN58R1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aX906RCv8R3g",
        "outputId": "fe3c969b-eb2d-4ef9-f7de-3544b36bca68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28.289752918178106"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ta_sents=\"அபுவின் நெடுங்காலப் பெருங்கனவு நிறைவேறி இருக்கிறது. இதில் அபு வேறுயாருமல்ல, இங்கிலாந்துக்கு எதிரான 3வது டெஸ்ட் போட்டியில் இந்திய அணியில் முதன் முதலாக களமிறங்கிய மும்பை பேட்டர் சர்ஃபராஸ் கானின் தந்தைதான்.தொடக்கப் போட்டியிலேயே சிறப்பாக ஆடி தந்தையின் கனவை நனவாக்கியுள்ளார் சர்ஃபராஸ் கான்.சர்ஃபராஸ் கானுக்கு தந்தையாகவும், சிறுவயதிலிருந்தே பயிற்சியாளராகவும், கிரிக்கெட்டில் வழிகாட்டியாகவும் இருந்து இந்திய அணிக்குள் செல்ல மூலகாரணமாக இருந்தவர் அபு என்ற நெளசத்கான் கான்.\""
      ],
      "metadata": {
        "id": "kmZDko7p8R65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1data=indic2en_model.translate_paragraph(ta_sents, 'ta', 'en')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0J8MGDD_mTC",
        "outputId": "a6bb58f1-c890-4866-fa39-4b6c06fc15d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 2366.32it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file2data=\"Abu's long-time dream has come true. Abu is none other than the father of Mumbai batsman Sarfaraz Khan, who made his debut for India in the 3rd Test against England. Sarfaraz Khan has fulfilled his father's dream by playing well in the opening match. Sarfaraz Khan has been a father, a coach and mentor to the Indian team since his childhood. The root cause was Nelasat Khan Khan alias Abu.\""
      ],
      "metadata": {
        "id": "Am82h0G3_mVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simi = findFileSimilarity(file1data,file2data )\n",
        "simi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvcrH-1L_mYz",
        "outputId": "69128ace-0b21-487b-b3d2-6939ef02f5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.53890708881835"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qqYEWFRtBI6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}